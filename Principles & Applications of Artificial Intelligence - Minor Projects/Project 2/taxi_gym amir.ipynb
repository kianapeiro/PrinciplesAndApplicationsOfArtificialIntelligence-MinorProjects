{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy_video(policy_table, name, video_width=500):\n",
    "\n",
    "    video_path = './video/' + name + '-step-0.mp4'\n",
    "        \n",
    "    env_test = gym.make('Taxi-v3', render_mode='rgb_array')\n",
    "    env_test = RecordVideo(env=env_test, video_folder=\"./video\", name_prefix=name, step_trigger=lambda x : True, disable_logger=True)\n",
    "\n",
    "    state, info = env_test.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    env_test.start_video_recorder()\n",
    "\n",
    "    while not done:\n",
    "        action = policy_table[state]\n",
    "        new_state, reward, done, truncated, info = env_test.step(int(action))\n",
    "        state = new_state\n",
    "        env_test.render()\n",
    "        steps += 1\n",
    "        if steps > 50:\n",
    "            break\n",
    "        \n",
    "    env_test.close_video_recorder()\n",
    "\n",
    "    env_test.close()\n",
    "\n",
    "    video_file = open(video_path, \"r+b\").read()\n",
    "    video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
    "    return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Taxi Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Taxi:\n",
    "    def __init__(self,\n",
    "            learning_rate=0.9, \n",
    "            discount_rate=0.8,\n",
    "            epsilon=1.0,\n",
    "            decay_rate=0.005,\n",
    "            num_iter=200,\n",
    "            num_episodes=10000,\n",
    "            max_steps=99,\n",
    "            num_evaluate_steps = 20):\n",
    "        self.env = gym.make('Taxi-v3') #initializing the environment\n",
    "        self.state_size = self.env.observation_space.n #size of state space (it is 500 in taxi environment)\n",
    "        self.action_size = self.env.action_space.n #size of action space (it is 6 in taxi environemt)\n",
    "        self.vtable = np.zeros(self.state_size) #initializing state value table\n",
    "        self.ptable = np.zeros(self.state_size) #initializing policy table (each entry contains one of 6 aciton)\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size)) #initializing q value table (value for each (state, action) pair)\n",
    "        self.learning_rate = learning_rate #learning rate used in Q-leanring algorithm\n",
    "        self.discount_rate = discount_rate #discount factor (gamma)\n",
    "        self.epsilon = epsilon #epsilon for specifying randomness threshold in epsilon-greedy action used in Q-learning\n",
    "        self.decay_rate = decay_rate #decay rate for decreasing the epsilon during the algorithm\n",
    "        self.num_iter = num_iter #number of iteration used in algorihtm\n",
    "        self.num_episodes = num_episodes #number of episodes (each episode is a sequence from start to end of a single game)\n",
    "        self.max_steps = max_steps #maximum bound for number of steps in each episode\n",
    "        self.num_evaluate_steps = num_evaluate_steps #number of steps to evaluate a given policy (just used in policy iteration algorithm)\n",
    "        \n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Initializing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Initialize the parameters by filling the blanks\n",
    "taxi = Taxi(num_iter=500, num_evaluate_steps=50, discount_rate=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Finding Optimal State Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use taxi.state_size and taxi.action_size\n",
    "# you can also use taxi.env.P[state][action] which returns a tuple containing (probability, next state, reward, if it is done or not)\n",
    "def value_iteration(vtable, num_iter, discount_rate):\n",
    "    for _ in range(num_iter):\n",
    "        v_old = np.copy(vtable)\n",
    "        for state in range(taxi.state_size) :\n",
    "            temp_action = np.zeros(taxi.action_size)\n",
    "            #TODO: compute the value for each action and fill the temp_action array with those values\n",
    "            for action in range(taxi.action_size):\n",
    "                for chance, next, reward, done in taxi.env.P[state][action]:\n",
    "                    temp_action[action] += chance * (reward + discount_rate * v_old[next])\n",
    "            vtable[state] = np.max(temp_action) #assigning the best action value to the state\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Extracting The Optimal Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy_extraction(vtable, ptable, num_iter, discount_rate):\n",
    "    value_iteration(vtable, num_iter, discount_rate)\n",
    "    for state in range(taxi.state_size) :\n",
    "        temp_action = np.zeros(taxi.action_size)\n",
    "        #TODO: compute the value for each action and fill the temp_action array with those values\n",
    "        for action in range(taxi.action_size):\n",
    "            for chance, next, reward, done in taxi.env.P[state][action]:\n",
    "                temp_action[action] += chance * (reward + discount_rate * vtable[next])\n",
    "        ptable[state] = np.argmax(temp_action) #finding the best action by argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Running The Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy_extraction(taxi.vtable, taxi.ptable, taxi.num_iter, taxi.discount_rate)\n",
    "optimal_policy = taxi.ptable.copy()     # saving determined optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Tesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No ffmpeg exe could be found. Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_policy_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaxi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue-iteration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[140], line 12\u001b[0m, in \u001b[0;36mtest_policy_video\u001b[0;34m(policy_table, name, video_width)\u001b[0m\n\u001b[1;32m      9\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     10\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[43menv_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_video_recorder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     15\u001b[0m     action \u001b[38;5;241m=\u001b[39m policy_table[state]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gymnasium/wrappers/record_video.py:134\u001b[0m, in \u001b[0;36mRecordVideo.start_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_video_recorder\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Starts video recorder using :class:`video_recorder.VideoRecorder`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose_video_recorder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     video_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-step-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_trigger:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gymnasium/wrappers/record_video.py:204\u001b[0m, in \u001b[0;36mRecordVideo.close_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_recorder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_recorder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorded_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gymnasium/wrappers/monitoring/video_recorder.py:149\u001b[0m, in \u001b[0;36mVideoRecorder.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorded_frames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmoviepy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImageSequenceClip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageSequenceClip\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mDependencyNotInstalled(\n\u001b[1;32m    152\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoviepy is not installed, run `pip install moviepy`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/moviepy/video/io/ImageSequenceClip.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imread\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mVideoClip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VideoClip\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mImageSequenceClip\u001b[39;00m(VideoClip):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    A VideoClip made from a series of images.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/moviepy/video/VideoClip.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mClip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Clip\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEVNULL, string_types\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_setting\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (add_mask_if_none, apply_to_mask,\n\u001b[1;32m     20\u001b[0m                           convert_masks_to_RGB, convert_to_seconds, outplace,\n\u001b[1;32m     21\u001b[0m                           requires_duration, use_clip_fps_by_default)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (deprecated_version_of, extensions_dict, find_extension,\n\u001b[1;32m     23\u001b[0m                      is_string, subprocess_call)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/moviepy/config.py:36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m FFMPEG_BINARY\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mffmpeg-imageio\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mffmpeg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_exe\n\u001b[0;32m---> 36\u001b[0m     FFMPEG_BINARY \u001b[38;5;241m=\u001b[39m \u001b[43mget_exe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m FFMPEG_BINARY\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto-detect\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m try_cmd([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mffmpeg\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/imageio/plugins/ffmpeg.py:173\u001b[0m, in \u001b[0;36mget_exe\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exe\u001b[39m():  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for imageio_ffmpeg.get_ffmpeg_exe()\"\"\"\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimageio_ffmpeg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_ffmpeg_exe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/imageio_ffmpeg/_utils.py:34\u001b[0m, in \u001b[0;36mget_ffmpeg_exe\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exe\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Nothing was found\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo ffmpeg exe could be found. Install ffmpeg on your system, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor set the IMAGEIO_FFMPEG_EXE environment variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No ffmpeg exe could be found. Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_policy_video(taxi.ptable, 'value-iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Initialize the parameters by filling the blanks\n",
    "taxi = Taxi(num_iter=500, num_evaluate_steps=50, discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(num_iter, discount_rate):\n",
    "    vtable = np.zeros(taxi.state_size)\n",
    "    for _ in range(num_iter):\n",
    "        v_old = np.copy(vtable)\n",
    "        for state in range(taxi.state_size) :\n",
    "            #TODO: compute the value of the state according to the current policy\n",
    "            action = taxi.ptable[state]\n",
    "            for chance, next, reward, done in taxi.env.P[state][action]:\n",
    "                vtable[state] += chance * (reward + discount_rate * v_old[next])\n",
    "    return vtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improvement(ptable, num_iter, num_evaluate_steps, discount_rate):\n",
    "    for _ in range(num_iter):\n",
    "        vtable = evaluate(num_evaluate_steps, discount_rate).copy()\n",
    "        for state in range(taxi.state_size) :\n",
    "            temp_action = np.zeros(taxi.action_size)\n",
    "            #TODO: compute the value for each action and fill the temp_action array with those values\n",
    "            for action in range(taxi.action_size):\n",
    "                for chance, next, reward, done in taxi.env.P[state][action]:\n",
    "                    temp_action[action] = temp_action + chance * (reward + discount_rate * vtable[next])\n",
    "            ptable[state] = np.argmax(temp_action) #improving the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Running The Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improvement(taxi.ptable, taxi.num_iter, taxi.num_evaluate_steps, taxi.discount_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_video(taxi.ptable, 'policy-iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Initializing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Initialize the parameters by filling the blanks\n",
    "taxi = Taxi(num_episodes=10000, max_steps=99, learning_rate=0.9, discount_rate=0.8, epsilon=1.0, decay_rate=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def q_learing_train(qtable, num_episodes, max_steps, learning_rate, discount_rate, epsilon, decay_rate):\n",
    "    for episode in range(num_episodes):\n",
    "        state, info = taxi.env.reset()\n",
    "        done = False\n",
    "        for s in range(max_steps):\n",
    "            #epsilon greedy\n",
    "            if random.uniform(0,1) < epsilon:\n",
    "                action = taxi.env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(qtable[state, :]) #TODO: assign the action for greedy case\n",
    "            new_state, reward, done, truncated, info = taxi.env.step(action) #doing one step\n",
    "            qtable[state][action] += learning_rate * (\n",
    "                reward + discount_rate * np.max(qtable[new_state, :]) - qtable[state][action]\n",
    "            )  #TODO: update q-value (main part of Q-learning)\n",
    "            state = new_state\n",
    "            if done == True:\n",
    "                break\n",
    "        epsilon = np.exp(-decay_rate*episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Runing The Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learing_train(taxi.qtable, taxi.num_episodes, taxi.max_steps, taxi.learning_rate, taxi.discount_rate, taxi.epsilon, taxi.decay_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Etracting The Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in range(taxi.state_size):\n",
    "    taxi.ptable[state] = np.argmax(taxi.qtable[state][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_video(taxi.ptable, 'Q-learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Direct Evaluation(Monte Carlo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the policy is given and you should just determine the value of each state based on the given policy. Then you can evaluate states, based on the optimal policy (you found in the previous sections) and then compare the 2 matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Initializing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Initialize the parameters by filling the blanks\n",
    "taxi = Taxi(num_episodes=10000, max_steps=99, discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo(ptable, num_episodes, max_steps, discount_rate):\n",
    "    count = np.ones(taxi.state_size) # counts the number of visits to each state\n",
    "    vtable = np.zeros(taxi.state_size) # store the total return for each state\n",
    "    for _ in range(num_episodes):\n",
    "        state, info = taxi.env.reset()\n",
    "        done = False\n",
    "        trajectory = [state]\n",
    "        rewards = []\n",
    "        for s in range(max_steps):\n",
    "            new_state, reward, done, truncated, info = taxi.env.step(ptable[state])\n",
    "            trajectory.append(new_state)\n",
    "            count[new_state] += 1\n",
    "            #TODO For each state visited in the current episode, update the value function vtable using the Monte Carlo update formula\n",
    "            rewards.append(reward)\n",
    "            count[new_state] += 1\n",
    "            state = new_state\n",
    "            if done == True:\n",
    "                break\n",
    "        A = 0\n",
    "        for i in reversed(range(len(trajectory))):\n",
    "            state = trajectory[i]\n",
    "            A = reward + discount_rate * A\n",
    "            if state not in trajectory[:i]: \n",
    "                vtable[state] += A\n",
    "    vtable /= count\n",
    "    return vtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_policy = np.zeros(taxi.state_size)\n",
    "given_policy_state_values = monte_carlo(given_policy, taxi.num_episodes, taxi.max_steps, taxi.discount_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy_state_values = monte_carlo(optimal_policy, taxi.num_episodes, taxi.max_steps, taxi.discount_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimal_policy_state_values - given_policy_state_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
